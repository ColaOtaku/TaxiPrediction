{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Dataset Project - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Steps\n",
    "\n",
    "**Step 0:** Prerequisites\n",
    "\n",
    "**Step 1:** Setup Spark cluster in AWS (do this from the shell/git bash)\n",
    "\n",
    "**Step 2:** Sanity check to ensure Spark & S3 are setup properly\n",
    "\n",
    "**Step 3:** Upload data files into S3 (you can skip this and use my S3 location)\n",
    "\n",
    "**Step 4:** Check cluster performance on dataset\n",
    "\n",
    "In the end I have also added other **uselful notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Step 1 is based on the CS109 [instructions](https://piazza.com/class/icf0cypdc3243c?cid=1369). However there are modifications for optimizing performance for this project\n",
    "\n",
    "### Step 0: Prerequisites\n",
    "\n",
    "1. You need the files CS109.pem and credentials.csv.If you had followed the cs109 instructions (for lab8 or HW5) you will already have these files.\n",
    "\n",
    "2. Create a new directory in your machine for the project and copy the following files into it:\n",
    "    \n",
    "    a) CS109.pem\n",
    "    \n",
    "    b) credentials.csv\n",
    "    \n",
    "    c) Setup Project.ipynb (this notebook)\n",
    "    \n",
    "    d) myConfig.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup Spark cluster in AWS & perform sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Note: Run all the items in Step 1 from unix shell/Git bash (not from the Jupyter notebook)**\n",
    "\n",
    "#### Step 1a) Create the cluster in AWS: run the following command in unix shell/Git bash (you may change the instance type/count)\n",
    "\n",
    "**Enhancements incorporated to the script**: \n",
    "1. Setup Spark to use the maximum available resources (the myConfig.json file has the instructions)\n",
    "2. Download the admin application Ganglia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export CLUSTER_ID=`aws emr create-cluster --name \"CS109 Proj Spark cluster\" \\\n",
    "--release-label emr-4.2.0 --applications Name=Spark Name=Ganglia --ec2-attributes KeyName=mykeypair \\\n",
    "--instance-type c3.2xlarge --instance-count 5 --configurations file://myConfig.json --use-default-roles \\\n",
    "--bootstrap-actions Path=s3://cs109-2015/install-anaconda-emr,Name=Install_Anaconda \\\n",
    "--query 'ClusterId' --output text` && echo $CLUSTER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1c) Wait for the cluster to be ready: AWS web console has to show \"WAITING\"\n",
    "\n",
    "#### Step 1d)  Get the cluster master's IP:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "export DNS_NAME=`aws emr describe-cluster --cluster-id $CLUSTER_ID \\\n",
    "--query 'Cluster.MasterPublicDnsName' --output text` && echo $DNS_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1e) Run the script to configure Spark "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ssh -o ServerAliveInterval=10 -i mykeypair.pem hadoop@$DNS_NAME 'sh configure-spark.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1f) Create an SSH tunel to the AWS box and connect to the cluster. This command assumes your SSH key is on the same directory you are invoking the SSH command from. At the end of this you will be in a terminal session on the cluster's master node."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ssh -o ServerAliveInterval=10 -i mykeypair.pem hadoop@$DNS_NAME -L 8989:localhost:8888"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1f) Open your browser and got to http://localhost:8989\n",
    "\n",
    "Note: The notebook you open will **already** have the spark context set up for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Sanity check to ensure Spark & S3 are setup properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a) Upload this Jupyter Notebook using the console from http://localhost:8989\n",
    "\n",
    "Note: all the steps in Step 2 are to be executed from the Jupyter Notebook iteself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b) Setup the SparkContext (automatically setup by YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f47346e3410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'yarn-client'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just an informational message\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2c) Spark sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "aa = rdd.map(lambda x: sys.version)\n",
    "aa.cache()\n",
    "aa.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2d) S3 sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop\n",
      "--2015-11-28 13:02:34--  http://en.wiktionary.org/wiki/awesome\n",
      "Resolving en.wiktionary.org (en.wiktionary.org)... 208.80.154.224, 2620:0:861:ed1a::1\n",
      "Connecting to en.wiktionary.org (en.wiktionary.org)|208.80.154.224|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 TLS Redirect\n",
      "Location: https://en.wiktionary.org/wiki/awesome [following]\n",
      "--2015-11-28 13:02:34--  https://en.wiktionary.org/wiki/awesome\n",
      "Connecting to en.wiktionary.org (en.wiktionary.org)|208.80.154.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘test_s3/awesome’\n",
      "\n",
      "awesome                 [ <=>                  ]  65.38K  --.-KB/s   in 0.002s \n",
      "\n",
      "2015-11-28 13:02:34 (31.8 MB/s) - ‘test_s3/awesome’ saved [66954]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get a wikipedia page and store it in a local folder\n",
    "!pwd\n",
    "!mkdir test_s3\n",
    "!wget http://en.wiktionary.org/wiki/awesome -P test_s3/ --trust-server-names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a S3 bucket : provide a unique name below: replace the \"testaaset1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: s3://testaaset1/\n",
      "upload: test_s3/awesome to s3://testaaset1/awesome\n"
     ]
    }
   ],
   "source": [
    "# Add the downloaded file to S3: remeber to replace \"testaaset1\" with a unique bucket name\n",
    "!aws s3 mb s3://testaaset1\n",
    "    \n",
    "# Add the downloaded file to the test bucket in S3: remeber to replace \"testaaset1\" with a unique bucket name\n",
    "!aws s3 cp test_s3/awesome s3://testaaset1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test if you are able to lookup the S3 file from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testS3RDD = sc.textFile(\"s3://testaaset1/awesome\")\n",
    "testS3RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congrats! Now you have a working spark cluster with ability to connect with S3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Upload data files into S3 \n",
    "\n",
    "Note: You **can skip** as i have uploaded the files to my S3 location ** s3://testsetu/nyc/  **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir datafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data files into local folder\n",
    "\n",
    "**Note**: We are downloading the data from **2013 onwards only** - though data is available from 2009\n",
    "\n",
    "**Data source**: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup the S3 bucket for storing the nyc data\n",
    "!aws s3 mb s3://testsetu/nyc\n",
    "    \n",
    "#Use the ephemerial/tmp space in EC2 node    \n",
    "!mkdir datafiles\n",
    "!sudo mount /dev/xvdb datafiles\n",
    "!mkdir datafiles/nyc\n",
    "\n",
    "#for green cabs\n",
    "!mkdir datafiles/nycg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setup the variables\n",
    "\n",
    "baseUrl = \"http://storage.googleapis.com/tlc-trip-data/\"\n",
    "#Yellow/green cab filename prefix\n",
    "yCabFNPrefix = \"/yellow_tripdata_\"\n",
    "gCabFNPrefix = \"/green_tripdata_\"\n",
    "\n",
    "#Availaiblity of data set by month & year\n",
    "yDict = {}\n",
    "gDict = {}\n",
    "\n",
    "#availablity for Yellow cab\n",
    "yDict[2015] = range(1,7) #available till jun 2015\n",
    "yDict[2014] = range(1,13)\n",
    "yDict[2013] = range(1,13)\n",
    "\n",
    "#availablity for Green cab\n",
    "gDict[2015] = range(1,7) #available till jun 2015\n",
    "gDict[2014] = range(1,13)\n",
    "gDict[2013] = range(8,13) #avialable only from august 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Yellow cab data file name list\n",
    "# file name is of format:  yellow_tripdata_2015-01.csv\n",
    "yCabUrls = []\n",
    "yCabFilenames = []\n",
    "for year, monthList in yDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = baseUrl+yearStr+yCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        yCabUrls.append(url)\n",
    "        yCabFilenames.append(yCabFNPrefix+yearStr+'-'+monthStr+\".csv\")\n",
    "\n",
    "#  green cab data file name list\n",
    "gCabUrls = []\n",
    "gCabFilenames = []\n",
    "for year, monthList in gDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = baseUrl+yearStr+gCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        gCabFilenames.append(gCabFNPrefix+yearStr+'-'+monthStr+\".csv\")\n",
    "        gCabUrls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download the yellow cab files\n",
    "for url in yCabUrls:\n",
    "    !wget $url -P datafiles/nyc --trust-server-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65G\tdatafiles/nyc\r\n"
     ]
    }
   ],
   "source": [
    "#Disk space of the Yellow Cab files\n",
    "!du -mh datafiles/nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(cabFilenames, isYellow):\n",
    "    \"\"\"\n",
    "    Function that takes a list of filenames (strings) and a boolean as parameters.\n",
    "    Removes the header from the each file and verifies the schema of the data.\n",
    "    \"\"\"\n",
    "    # Dictionary where key = filename, value = (schema, bool==True if there is a blank line after header)\n",
    "    file_schemas = {}\n",
    "    prefix = 'datafiles/nycg/'\n",
    "    if isYellow:\n",
    "        prefix = 'datafiles/nyc/'\n",
    "        \n",
    "    for filename in cabFilenames:\n",
    "        # Fetch schema\n",
    "        with open(prefix+filename,'r') as in_fp:\n",
    "            #read first two lines\n",
    "            lines = [in_fp.readline() for i in xrange(2)]\n",
    "\n",
    "        # now open again to write out\n",
    "        file_schemas[filename] = (tuple(lines[0].split(',')), lines[1]=='\\r\\n')\n",
    "    \n",
    "    # verify all files have the necessary columns in the same position\n",
    "    for (schema,blank) in file_schemas.values():\n",
    "        assert 'ickup' in schema[1]\n",
    "        assert 'atetime' in schema[1]\n",
    "        assert 'ickup' in schema[5]\n",
    "        assert 'ongitude' in schema[5]\n",
    "        assert 'ickup' in schema[6]\n",
    "        assert 'atitude' in schema[6]\n",
    "    print \"Schema:\", file_schemas[filename][0]\n",
    "    \n",
    "    # Remove header and blank line from file\n",
    "    for filename in cabFilenames:\n",
    "        print \"Writing to %r\" % filename \n",
    "        with open(prefix+filename,'r') as in_fp:\n",
    "            #read whole file\n",
    "            lines = in_fp.readlines()\n",
    "\n",
    "        with open(prefix+filename,'w') as out_fp:\n",
    "\n",
    "            # check if there is a blank line after the header\n",
    "            if file_schemas[filename][1]:\n",
    "                out_fp.writelines(lines[2:])\n",
    "            else:\n",
    "                out_fp.writelines(lines[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess Yellow Cab files -- check schema\n",
    "preprocess_data(yCabFilenames, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add to s3\n",
    "!aws s3 sync datafiles/nyc/ s3://testsetu/nyc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free up space\n",
    "!rm datafiles/nyc/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download the green cab files\n",
    "for url in gCabUrls:\n",
    "    !wget $url -P datafiles/nycg --trust-server-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0G\tdatafiles/nycg\r\n"
     ]
    }
   ],
   "source": [
    "#Disk space of the Yellow Cab files\n",
    "!du -mh datafiles/nycg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: ('VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime', 'Store_and_fwd_flag', 'RateCodeID', 'Pickup_longitude', 'Pickup_latitude', 'Dropoff_longitude', 'Dropoff_latitude', 'Passenger_count', 'Trip_distance', 'Fare_amount', 'Extra', 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Ehail_fee', 'Total_amount', 'Payment_type', 'Trip_type \\n')\n",
      "Writing to 'green_tripdata_2013-10.csv'\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Green Cab files\n",
    "preprocess_data(gCabFilenames, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync datafiles/nycg/ s3://testsetu/nyc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#free up space\n",
    "!rm datafiles/nycg/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Check cluster performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.84 ms\n",
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 14.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12450522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.textFile(\"s3://testsetu/nyc/yellow_tripdata_2015-02.csv\")\n",
    "%time myRDD.cache()\n",
    "%time myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 4.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12450522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Enable the web admin interface** from the AWS console (follow the steps it says). Note: in this step when you open the SSH conection (as per instructions), the connection might not show ANY thing status etc) - this is fine. The SSH command (As per instruction) is:  ssh -i CS109.pem $DNS_NAME -ND 8157\n",
    "\n",
    "**2. Admin UI's**\n",
    "    \n",
    "    a) To get to the Spark Jobs Admin Console: Go to the Hadoop Resource Manager UI (from AWS console) and click on \"Application master\" link (it will be one of the items in the listed running applications).\n",
    "    \n",
    "    b) Spark history server: http://<domain>:18080/\n",
    "    \n",
    "    c) For CPU/Memory performance on each node use the Ganglia UI (link from the AWS console)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
