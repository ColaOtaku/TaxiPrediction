{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Dataset Project - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Steps\n",
    "\n",
    "**Step 0:** Prerequisites\n",
    "\n",
    "**Step 1:** Start Spark Cluster\n",
    "\n",
    "**Step 2:** Upload this notebook and load data from s3\n",
    "\n",
    "**Step 3:** Clean Data\n",
    "\n",
    "**Step 4:** Aggregate Data\n",
    "\n",
    "**Step 5:** Save RDD to file on s3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Step 1 is based on the CS109 [instructions](https://piazza.com/class/icf0cypdc3243c?cid=1369). However there are modifications for optimizing performance for this project\n",
    "\n",
    "### Step 0: Prerequisites\n",
    "\n",
    "1. You need the files CS109.pem and credentials.csv.If you had followed the cs109 instructions (for lab8 or HW5) you will already have these files.\n",
    "\n",
    "2. You will need a directory containing the following files:\n",
    "    \n",
    "    a) CS109.pem\n",
    "    \n",
    "    b) credentials.csv\n",
    "    \n",
    "    c) Setup Project.ipynb\n",
    "    \n",
    "    d) myConfig.json\n",
    "    \n",
    "    e) DataPrep.ipynb (this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Start Spark cluster and sanity check\n",
    "\n",
    "#### Step 1a) Start your Spark cluster as described in Step 1 from Setup Project (unless your spark cluster is already running)\n",
    "#### Step 1b) Sanity check: make sure Spark cluster is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "aa = rdd.map(lambda x: sys.version)\n",
    "aa.cache()\n",
    "aa.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload this notebook and load data from s3\n",
    "\n",
    "#### Upload this Jupyter Notebook using the console from http://localhost:8989\n",
    "\n",
    "Notes: \n",
    "1. All the steps in Step 2 are to be executed from the Jupyter Notebook iteself\n",
    "2. We will frequently be loading data form the s3 bucket you created in Step 3 of Setup Project (I will use the bucket name: \"sdaultontestbucket\", but replace this with your own\n",
    "3. All the steps in Step 2 are to be executed from the Jupyter Notebook iteself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather data filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setup the variables\n",
    "\n",
    "#Yellow/green cab filename prefix\n",
    "yCabFNPrefix = \"yellow_tripdata_\"\n",
    "gCabFNPrefix = \"green_tripdata_\"\n",
    "\n",
    "#Availaiblity of data set by month & year\n",
    "yDict = {}\n",
    "gDict = {}\n",
    "\n",
    "#availablity for Yellow cab\n",
    "yDict[2015] = range(1,7) #available till jun 2015\n",
    "yDict[2014] = range(1,13)\n",
    "yDict[2013] = range(1,13)\n",
    "\n",
    "#availablity for Green cab\n",
    "gDict[2015] = range(1,7) #available till jun 2015\n",
    "gDict[2014] = range(1,13)\n",
    "gDict[2013] = range(8,13) #avialable only from august 2013\n",
    "\n",
    "# Yellow cab data file name list\n",
    "# file name is of format:  yellow_tripdata_2015-01.csv\n",
    "yCabUrls = []\n",
    "for year, monthList in yDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = yCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        yCabUrls.append(url)\n",
    "\n",
    "#  green cab data file name list\n",
    "gCabUrls = []\n",
    "for year, monthList in gDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = gCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        gCabUrls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Yellow Cab Data\n",
    "1. Create an RDD to store all taxi data\n",
    "2. Get the schema of the data file\n",
    "3. Get Relevant Data:\n",
    "    \n",
    "    a. Pickup datetime\n",
    "    \n",
    "    b. Pickup latitude\n",
    "    \n",
    "    c. Pickup longitude\n",
    "4. Round latitude and longitude to discretize locations\n",
    "5. Get day of the week and hour for each pickup\n",
    "5. Calculate the number of pickups per (day of the week, hour, location)\n",
    "6. Aggregate data from all datafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an RDD to store all taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.98 ms\n",
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 19.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14776617"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_rdd = sc.textFile(\"s3://sdaultontestbucket/nyc/\"+yCabUrls[0])\n",
    "%time taxi_rdd.cache()\n",
    "%time taxi_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the schema of the data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'vendor_id', u'pickup_datetime', u'dropoff_datetime', u'passenger_count', u'trip_distance', u'pickup_longitude', u'pickup_latitude', u'rate_code', u'store_and_fwd_flag', u'dropoff_longitude', u'dropoff_latitude', u'payment_type', u'fare_amount', u'surcharge', u'mta_tax', u'tip_amount', u'tolls_amount', u'total_amount')\n"
     ]
    }
   ],
   "source": [
    "taxi_rdd = taxi_rdd.map(lambda line: tuple(line.split(','))).zipWithIndex()\n",
    "schema = taxi_rdd.take(1)[0][0]\n",
    "print schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_indices(schema):\n",
    "    # Takes a list of column names (strings) as a parameter and returns a tuple of the indices of the pickup datetime,\n",
    "    # pickup latitude, and pickup longitude columns\n",
    "    indices = [-1,-1,-1]\n",
    "    for idx in xrange(len(schema)):\n",
    "        col_name = schema[idx]\n",
    "        if \"pickup\" in col_name:\n",
    "            if \"datetime\" in col_name:\n",
    "                indices[0] = idx\n",
    "            elif \"latitude\" in col_name:\n",
    "                indices[1] = idx\n",
    "            elif \"longitude\" in col_name:\n",
    "                indices[2] = idx\n",
    "    return tuple(indices)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "def date_extractor(date_str):\n",
    "    # Takes a datetime object as a parameter\n",
    "    # and extracts and returns a tuple of the day of the week (1 through 7 where Monday == 1) and hour (1 through 24)\\\n",
    "    \n",
    "    # Split date string into list of date, time\n",
    "    d = date_str.split()\n",
    "    # Parse year, month, day\n",
    "    date_list = d[0].split('-')\n",
    "    d_obj = date(int(date_list[0]),int(date_list[1]),int(date_list[2]))\n",
    "    day_of_week = d_obj.isoweekday()\n",
    "    # Get hour number\n",
    "    hour = int(d[1].split(':')[0]) + 1\n",
    "    return (day_of_week, hour)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import geohash\n",
    "def data_cleaner(row):\n",
    "    # takes a tuple (row,idx) as a parameter and returns a tuple of the form:\n",
    "    # (day of the week, hour, geotag)\n",
    "    indices = (1, 6, 5)\n",
    "    #deal with header\n",
    "    #assert len(row_with_idx) == 2, \"row_with_idx is len %r\" % len(row_with_idx)\n",
    "    #if row_with_idx[1] == 0:\n",
    "    #    return (-1,-1,0)\n",
    "    #else:\n",
    "    #    row = row_with_idx[0]\n",
    "    #assert len(row) > 6, \"row is len %r\" % len(row)\n",
    "    #extract day of the week and hour\n",
    "    date_str = row[indices[0]]\n",
    "    clean_date = date_extractor(date_str)\n",
    "    #assert len(clean_date) == 2, \"clean date is len %r\" % len(clean_date)\n",
    "    #get geo hash\n",
    "    latitude = float(row[indices[1]])\n",
    "    longitude = float(row[indices[2]])\n",
    "    \n",
    "    #location = geohash.encode(latitude,longitude)\n",
    "    # I was having trouble importing geohash on AWS, so for now we round to 3 decimal places to discretize the data\n",
    "    # At New York's location,\n",
    "    # .001 degree latitude is approximately 100 meters\n",
    "    # .001 degree longitude is less than 100 meters\n",
    "    location = (round(latitude,3), round(longitude,3))\n",
    "\n",
    "    return (clean_date[0], clean_date[1], location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the relevant indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "indices = fetch_indices(schema)\n",
    "assert (-1 not in indices)\n",
    "assert indices == (1,6,5)\n",
    "print indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of header row and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "taxi_rdd = taxi_rdd.filter(lambda (row,idx): idx > 1).map(lambda (row,idx): row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_rdd = taxi_rdd.map(data_cleaner)\\\n",
    "                .map(lambda row: (row,1))\\\n",
    "                .reduceByKey(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_rdd.cache()\n",
    "taxi_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summer(val):\n",
    "    # takes a tuple (with 2 elements) val as a parameter and returns the sum of the two elements if both or not None\n",
    "    # Otherwise returns the element that is not none\n",
    "    if val[0] is None:\n",
    "        assert val[1] is not None\n",
    "        return val[1]\n",
    "    elif val[1] is None:\n",
    "        assert val[0] is not None\n",
    "        return val[0]\n",
    "    \n",
    "    return val[0]+val[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Yellow Cab Data from all datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "failed_to_read = []\n",
    "for i in xrange(1,len(yCabUrls)):\n",
    "    filename = yCabUrls[i]\n",
    "    temp_rdd = sc.textFile(\"s3://sdaultontestbucket/nyc/\"+filename)\n",
    "    temp_rdd.cache()\n",
    "    temp_rdd = temp_rdd.map(lambda line: tuple(line.split(','))).zipWithIndex()\n",
    "    schema = temp_rdd.take(1)[0][0]\n",
    "    indices = fetch_indices(schema)\n",
    "    # Make sure fetch indices found all the columns\n",
    "    if (-1 in indices) or (indices != (1,6,5)):\n",
    "        failed_to_read.append(\"s3://sdaultontestbucket/nyc/\"+filename)\n",
    "    else:\n",
    "        # Get rid of header row and clean the data\n",
    "        temp_rdd = temp_rdd.filter(lambda (row,idx): idx > 1).map(lambda (row,idx): row)\n",
    "        # Clean data and reduce data down to ((day of week, hour, location), number of pickups) tuples\n",
    "        temp_rdd = temp_rdd.map(data_cleaner)\\\n",
    "                    .map(lambda row: (row,1))\\\n",
    "                    .reduceByKey(lambda a,b: a+b)\n",
    "        #Add rows to whole dataset\n",
    "        # this join gives us (key, (count from taxi, count from temp))\n",
    "\n",
    "        taxi_rdd = taxi_rdd.fullOuterJoin(temp_rdd).mapValues(summer)\n",
    "        taxi_rdd.cache()\n",
    "        print taxi_rdd.count()\n",
    "\n",
    "    temp_rdd.unpersist()\n",
    "    print \"Read \"+str(i)+\" of \"+str(len(yCabUrls)-1)+ \" files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the RDD as a single file\n",
    "# this RDD has key = (day of week, hour, location), value = number of pickups\n",
    "taxi_rdd.repartition(1).saveAsTextFile(\"s3n://sdaultontestbucket/summed_rdd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
