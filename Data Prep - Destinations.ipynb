{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fca81b70490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import geohash\n",
    "\n",
    "from datetime import *\n",
    "from dateutil.parser import parse\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"geohash.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2: pickup dattime\n",
    "    6: pickup long\n",
    "    7: pickup lat\n",
    "    10: dropoff long\n",
    "    11: dropoff lat\n",
    "\"\"\"\n",
    "def yCabParse(strRecord):    \n",
    "    return (parse(strRecord[2]), float(strRecord[6]), float(strRecord[7]), float(strRecord[10]), float(strRecord[11]))\n",
    "\n",
    "yCabRDD = sc.textFile(\"s3://testsetu/nyc/final/yellow/consolidated/pa*\").map(lambda line: tuple(line.split(',')))\n",
    "yCabRDD = yCabRDD.map(lambda record: yCabParse(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2: pickup dattime\n",
    "    6: pickup long\n",
    "    7: pickup lat\n",
    "    8: dropoff long\n",
    "    9: dropoff lat\n",
    "\"\"\"\n",
    "def gCabParse(strRecord):    \n",
    "    return (parse(strRecord[2]), float(strRecord[6]), float(strRecord[7]), float(strRecord[8]), float(strRecord[9]))\n",
    "\n",
    "gCabRDD = sc.textFile(\"s3://testsetu/nyc/final/green/consolidated/pa*\").map(lambda line: tuple(line.split(',')))\n",
    "gCabRDD = gCabRDD.map(lambda record: gCabParse(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combinedRDD = yCabRDD.union(gCabRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2: pickup dattime\n",
    "    6: pickup long\n",
    "    7: pickup lat\n",
    "    8: dropoff long\n",
    "    9: dropoff lat\n",
    "\"\"\"\n",
    "def prepData(record):\n",
    "    \n",
    "    geohashAccuracy = 6\n",
    "    minsPerBin = 48\n",
    "    \n",
    "    pickupDatetime = record[0] \n",
    "    pickupLong = record[1]\n",
    "    pickupLat = record[2]\n",
    "    dropOffLong = record[3]\n",
    "    dropOffLat = record[4]\n",
    "    \n",
    "    if pickupLat < 50 and pickupLat > 35 and pickupLong < -50 and pickupLong > -80:\n",
    "        pickupGeohash = geohash.encode(pickupLat,pickupLong, geohashAccuracy)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    if dropOffLat < 50 and dropOffLat > 35 and dropOffLong < -50 and dropOffLong > -80:\n",
    "        dropOffGeohash = geohash.encode(dropOffLat,dropOffLong, geohashAccuracy)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    #time_cat\n",
    "    d = pickupDatetime\n",
    "    \n",
    "    totalMinsPerDay = 1440\n",
    "    totalBins = totalMinsPerDay/minsPerBin\n",
    "    \n",
    "    elapsMins = (d.hour)*60 + d.minute\n",
    "    #minsPerBin = totalMinsPerDay/totalBins\n",
    "    currentBin = elapsMins/minsPerBin\n",
    "    binnedHour = d.hour #elapsMins/60\n",
    "    binnedMin = (currentBin * minsPerBin)- (binnedHour * 60)\n",
    "    \n",
    "    binStr = \"\"\n",
    "    \n",
    "    if (binnedHour/10>0):\n",
    "        binStr = str(binnedHour)\n",
    "    else:\n",
    "        binStr = \"0\"+str(binnedHour)\n",
    "    \n",
    "    binStr = binStr + \":\"\n",
    "    \n",
    "    if (binnedMin/10>0):\n",
    "        binStr = binStr + str(binnedMin)\n",
    "    else:\n",
    "        binStr = binStr + \"0\"+str(binnedMin)\n",
    "    \n",
    "    time_num = (binnedHour*60 + binnedMin + minsPerBin / 2.0)/(60*24)  \n",
    "    \n",
    "    #day of week\n",
    "    dayStr = {0: \"Mon\",\n",
    "                  1: \"Tue\",\n",
    "                  2: \"Wed\",\n",
    "                  3: \"Thu\",\n",
    "                  4: \"Fri\",\n",
    "                  5: \"Sat\",\n",
    "                  6: \"Sun\"}\n",
    "    day_of_week = dayStr[d.weekday()]\n",
    "    \n",
    "    #weekend\n",
    "    \"\"\"if d.weekday() in [5,6]:\n",
    "        weekend = 1\n",
    "    else:\n",
    "        weekend = 0\"\"\"\n",
    "    \n",
    "    return ((pickupGeohash, dropOffGeohash,time_num,d.weekday()),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedCleanRDD = combinedRDD.map(lambda record: prepData(record)).filter(lambda a: a is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedRDD = combinedCleanRDD.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time groupedRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def toCSVLine(record):\n",
    "    data = [record[0][0], record[0][1], record[0][2], record[0][3], record[1]]\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "csvRDD = groupedRDD.map(toCSVLine)\n",
    "csvRDD.repartition(1).saveAsTextFile('s3://testsetu/nyc/final/groupbydestn/singlefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dr5rtk,dr5rsj,0.416666666667,4,2',\n",
       " u'dr5x0z,dr5xcf,0.683333333333,0,2',\n",
       " u'dr72rd,dr782h,0.65,3,2',\n",
       " u'dr5rvn,dr5rt5,0.0833333333333,2,11',\n",
       " u'dr72jd,dr72m3,0.05,0,3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"s3://testsetu/nyc/final/groupbydestn/singlefile/pa*\").map(lambda line: line).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pickupGeohash, dropOffGeohash,time_num,day_of_week, count\n",
    "\"\"\"\n",
    "def groupedParse(strRecord):    \n",
    "    return (strRecord[0], strRecord[1], float(strRecord[2]), int(strRecord[3]), int(strRecord[4]))\n",
    "\n",
    "gpRDD = sc.textFile(\"s3://testsetu/nyc/final/groupbydestn/singlefile/pa*\").map(lambda line: tuple(line.split(','))).map(lambda x: groupedParse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'dr5rtk', u'dr5rsj', 0.416666666667, 4, 2),\n",
       " (u'dr5x0z', u'dr5xcf', 0.683333333333, 0, 2)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15285988"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpRDD.cache()\n",
    "gpRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropoffGeohashes = gpRDD.map(lambda x: x[1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictLength = len(dropoffGeohashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropoffGeohashDict = {}\n",
    "i = 0\n",
    "for gh in dropoffGeohashes:\n",
    "    dropoffGeohashDict[gh] = i\n",
    "    i = i +1\n",
    "\n",
    "broadcastGH = sc.broadcast(dropoffGeohashDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import math\n",
    "#Create features as labeledpoint\n",
    "\n",
    "\"\"\"\n",
    "0:pickupGeohash\n",
    "1:dropOffGeohash\n",
    "2:time_num\n",
    "3:day_of_week\n",
    "4: count\n",
    "\"\"\"\n",
    "def extractFeaturesforML(record):\n",
    "    #np.array([1.0, 0.0, 3.0])\n",
    "    \n",
    "    count = record[4]\n",
    "    timeCos = math.cos(record[2] * 2 * math.pi)\n",
    "    timeSin = math.sin(record[2] * 2 * math.pi)\n",
    "    \n",
    "    #dayCos = math.cos(key[3] * 2 * math.pi)\n",
    "    #daySin = math.sin(key[3] * 2 * math.pi)\n",
    "    \n",
    "    pickupLatLong = geohash.decode(record[0])\n",
    "    pickupLat=pickupLatLong[0]\n",
    "    pickupLong=pickupLatLong[1]    \n",
    "    \n",
    "    features_ = np.array([record[2], record[3], count, timeCos, timeSin, pickupLat, pickupLong])\n",
    "    \n",
    "    return LabeledPoint(broadcastGH.value[record[1]], features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(26847.0, [0.416666666667,4.0,2.0,-0.866025403785,0.499999999998,40.7180786133,-73.9434814453]),\n",
       " LabeledPoint(7859.0, [0.683333333333,0.0,2.0,-0.406736643078,-0.913545457642,40.6466674805,-73.7896728516])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpRDD.map(lambda x: extractFeaturesforML(x)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15285988"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresLP = gpRDD.map(lambda x: extractFeaturesforML(x)).cache()\n",
    "%time featuresLP.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.98894831021\n",
      "Learned classification forest model:\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = featuresLP.randomSplit([0.8, 0.2])\n",
    "\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=dictLength, categoricalFeaturesInfo={},\n",
    "                                     numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=10, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "#print(model.toDebugString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
